{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing and cleaning are crucial steps in the machine learning pipeline. They involve transforming raw data into a suitable format that can be effectively used by machine learning algorithms. The quality and integrity of the data significantly impact the performance and reliability of the trained models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance of data preprocessing and cleaning in ML:\n",
    "- Ensures data consistency and reliability\n",
    "- Reduces noise and irrelevant information\n",
    "- Handles missing values and outliers\n",
    "- Normalizes and scales features\n",
    "- Encodes categorical variables\n",
    "- Improves model performance and generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common data quality issues:\n",
    "1. Missing values\n",
    "   - Occurs when certain features or observations have no recorded value\n",
    "   - Can be due to data collection issues, system failures, or human errors\n",
    "\n",
    "2. Outliers\n",
    "   - Data points that significantly deviate from the majority of the data\n",
    "   - Can be caused by measurement errors, data entry mistakes, or genuine extreme values\n",
    "\n",
    "3. Inconsistent formatting\n",
    "   - Variations in data formats, such as date formats or units of measurement\n",
    "   - Requires standardization for consistent processing\n",
    "\n",
    "4. Duplicate entries\n",
    "   - Repeated instances of the same data point\n",
    "   - Need to be identified and handled appropriately\n",
    "\n",
    "5. Incorrect or invalid data\n",
    "   - Data points that violate domain-specific rules or constraints\n",
    "   - For example, negative age values or invalid zip codes\n",
    "\n",
    "6. Imbalanced data\n",
    "   - Unequal representation of different classes or categories in the dataset\n",
    "   - Can lead to biased models that perform poorly on underrepresented classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addressing these data quality issues through proper preprocessing and cleaning techniques is essential to ensure the reliability and effectiveness of the machine learning models built on top of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will explore various techniques and approaches to handle missing data, outliers, data transformation, imbalanced datasets, and more, using Python and popular data manipulation libraries such as NumPy and Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Handling Missing Data](#toc1_)    \n",
    "- [Handling Outliers](#toc2_)    \n",
    "- [Data Transformation](#toc3_)    \n",
    "- [Handling Duplicate Entries](#toc4_)    \n",
    "- [Handling Incorrect or Invalid Data](#toc5_)    \n",
    "- [Handling Imbalanced Data](#toc6_)    \n",
    "- [Summary](#toc7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Handling Missing Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data is a common issue in real-world datasets. It occurs when certain features or observations have no recorded value. Identifying and handling missing data is crucial to ensure the quality and reliability of the machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying missing values:\n",
    "- In Python, missing values are typically represented as `None`, `NaN` (Not a Number), or `NA` (Not Available) depending on the data type and library used.\n",
    "- Pandas provides functions like `isnull()` and `isna()` to identify missing values in a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame with missing values\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8],\n",
    "        'C': [9, 10, 11, None]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       A      B      C\n",
       "0  False  False  False\n",
       "1  False   True  False\n",
       "2   True  False  False\n",
       "3  False  False   True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify missing values\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techniques for dealing with missing data:\n",
    "\n",
    "1. Deletion\n",
    "   - Listwise deletion (complete case analysis): Remove entire rows containing missing values.\n",
    "   - Pairwise deletion (available case analysis): Remove only the specific missing values, keeping the rest of the data intact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Listwise deletion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C\n",
       "0  1.0  5.0  9.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with missing values\n",
    "df_listwise = df.dropna()\n",
    "df_listwise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Imputation\n",
    "   - Mean/Median imputation: Replace missing values with the mean or median of the available data for that feature.\n",
    "   - Mode imputation: Replace missing values with the most frequent value (mode) of the available data for that feature.\n",
    "   - KNN imputation: Use the K-Nearest Neighbors algorithm to estimate missing values based on similar instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Mean imputation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.333333</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B     C\n",
       "0  1.000000  5.000000   9.0\n",
       "1  2.000000  6.666667  10.0\n",
       "2  2.333333  7.000000  11.0\n",
       "3  4.000000  8.000000  10.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace missing values with the mean of each column\n",
    "df_mean_imputed = df.fillna(df.mean())\n",
    "df_mean_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing a technique for handling missing data, consider the following:\n",
    "- The amount and pattern of missing data (random or systematic)\n",
    "- The nature of the data (numerical, categorical)\n",
    "- The potential impact on the analysis and model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to carefully evaluate the implications of each technique and choose the most appropriate approach based on the specific characteristics of your dataset and the requirements of your machine learning task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will explore techniques for handling outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Handling Outliers](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are data points that significantly deviate from the majority of the data. They can be caused by measurement errors, data entry mistakes, or genuine extreme values. Outliers can have a significant impact on statistical analyses and machine learning models, leading to biased or skewed results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and detecting outliers:\n",
    "- Outliers are typically defined based on a certain threshold or statistical measure, such as the interquartile range (IQR) or standard deviation.\n",
    "- Common methods for detecting outliers include:\n",
    "  - Box plots: Visualize the distribution of data and identify points outside the whiskers.\n",
    "  - Z-score: Measure how many standard deviations a data point is from the mean.\n",
    "  - Isolation Forest: An unsupervised learning algorithm that isolates anomalies based on their distance from other data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Detecting outliers using Z-score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: [1053]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with outliers\n",
    "data = np.array([[92, 19, 101, 58, 1053, 91, 26, 78, 10, 13, -40, 101, 86, 85, 15, 89, 89, 28, -5, 41]])\n",
    "\n",
    "# Calculate the Z-score for each data point\n",
    "z_scores = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "# Define a threshold for outliers (e.g., Z-score > 2)\n",
    "threshold = 3\n",
    "\n",
    "# Identify outliers based on the threshold\n",
    "outliers = data[np.abs(z_scores) > threshold]\n",
    "print(\"Outliers:\", outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techniques for handling outliers:\n",
    "\n",
    "1. Deletion\n",
    "   - Remove the outliers from the dataset if they are considered erroneous or irrelevant.\n",
    "   - Be cautious when deleting outliers, as they may contain valuable information in some cases.\n",
    "\n",
    "2. Transformation\n",
    "   - Apply mathematical transformations to reduce the impact of outliers.\n",
    "   - Common transformations include logarithmic, square root, or reciprocal transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Logarithmic transformation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data: [0.         0.69314718 1.09861229 1.38629436 1.60943791 1.79175947\n",
      " 1.94591015 2.07944154 2.19722458 4.60517019]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with outliers\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 100])\n",
    "\n",
    "# Apply logarithmic transformation\n",
    "transformed_data = np.log(data)\n",
    "print(\"Transformed data:\", transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Winsorization\n",
    "   - Replace the outliers with the nearest \"normal\" value, such as the 5th or 95th percentile.\n",
    "   - Winsorization preserves the information that an outlier exists while limiting its impact on the overall analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Winsorization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winsorized data: [ 92  19 101  58 101  91  26  78  10  13  -5 101  86  85  15  89  89  28\n",
      "  -5  41]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Create a sample dataset with outliers\n",
    "data = np.array([92, 19, 101, 58, 1053, 91, 26, 78, 10, 13, -40, 101, 86, 85, 15, 89, 89, 28, -5, 41])\n",
    "\n",
    "# Perform winsorization (replace outliers with 5th and 95th percentiles)\n",
    "winsorized_data = stats.mstats.winsorize(data, limits=[0.05, 0.05])\n",
    "print(\"Winsorized data:\", winsorized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with outliers, consider the following:\n",
    "- Understand the nature and source of the outliers (errors, anomalies, or genuine extreme values).\n",
    "- Assess the impact of outliers on your analysis and model performance.\n",
    "- Choose an appropriate technique based on the characteristics of your data and the requirements of your machine learning task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that outliers can sometimes provide valuable insights and should not be blindly removed without careful consideration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will explore data transformation techniques, including normalization, standardization, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Data Transformation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation is the process of converting data from one format or structure into another. It is a crucial step in data preprocessing to ensure that the data is in a suitable format for analysis and machine learning algorithms. Common data transformation techniques include normalization, standardization, and encoding categorical variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalization and Standardization\n",
    "   - Normalization scales the data to a specific range, typically between 0 and 1.\n",
    "   - Standardization transforms the data to have zero mean and unit variance.\n",
    "   - These techniques help to ensure that all features have similar scales and prevent certain features from dominating others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Min-Max Normalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/a5/53/c7b76a9aa241536635037a7956be36a0c2718262c234085815e8000e9ec6/scikit_learn-1.4.1.post1-cp310-cp310-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/1e/84/ccd9b08653022b7785b6e3ee070ffb2825841e0dc119be22f0840b2b35cb/threadpoolctl-3.4.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.1.post1-cp310-cp310-macosx_12_0_arm64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m734.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.4.1.post1 threadpoolctl-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data:\n",
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a sample dataset\n",
    "data = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "print(\"Normalized data:\")\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encoding Categorical Variables\n",
    "   - Machine learning algorithms typically require numerical inputs.\n",
    "   - Categorical variables need to be encoded into numerical representations.\n",
    "   - Common encoding techniques include:\n",
    "     - One-Hot Encoding: Creates binary dummy variables for each category.\n",
    "     - Label Encoding: Assigns a unique numerical label to each category.\n",
    "     - Ordinal Encoding: Assigns numerical labels based on the order or hierarchy of categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (One-Hot Encoding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data:\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a sample dataset with categorical variables\n",
    "data = [['red'], ['green'], ['blue'], ['red'], ['green']]\n",
    "\n",
    "# Create a OneHotEncoder object\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoded_data = encoder.fit_transform(data).toarray()\n",
    "print(\"Encoded data:\")\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying data transformation techniques, consider the following:\n",
    "- Choose the appropriate technique based on the nature of your data and the requirements of your machine learning algorithm.\n",
    "- Be cautious when applying normalization or standardization to avoid losing important information or introducing biases.\n",
    "- Consider the interpretability and domain-specific meaning of the transformed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent and appropriate data transformation ensures that the data is in a suitable format for machine learning algorithms to process effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will discuss techniques for handling imbalanced datasets, where the distribution of classes or categories is uneven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Handling Duplicate Entries](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate entries refer to repeated instances of the same data point in a dataset. Duplicate data can occur due to various reasons, such as data entry errors, data collection issues, or merging datasets from multiple sources. Identifying and handling duplicate entries is important to ensure data integrity and avoid biased or misleading results in data analysis and machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying duplicate entries:\n",
    "- In Python, you can use the `duplicated()` function from the Pandas library to identify duplicate rows in a DataFrame.\n",
    "- The `duplicated()` function returns a boolean series indicating which rows are duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows:\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with duplicate entries\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 2, 4],\n",
    "                   'B': [5, 6, 7, 6, 8],\n",
    "                   'C': [9, 10, 11, 10, 12]})\n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicate_rows = df.duplicated()\n",
    "print(\"Duplicate rows:\")\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling duplicate entries:\n",
    "1. Removing duplicates\n",
    "   - If duplicate entries are considered redundant or unnecessary, you can remove them from the dataset using the `drop_duplicates()` function in Pandas.\n",
    "   - By default, `drop_duplicates()` keeps the first occurrence of each unique row and removes the subsequent duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after removing duplicates:\n",
      "   A  B   C\n",
      "0  1  5   9\n",
      "1  2  6  10\n",
      "2  3  7  11\n",
      "4  4  8  12\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows\n",
    "df_unique = df.drop_duplicates()\n",
    "print(\"DataFrame after removing duplicates:\")\n",
    "print(df_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Keeping specific occurrences\n",
    "   - In some cases, you may want to keep a specific occurrence of a duplicate entry, such as the last occurrence instead of the first.\n",
    "   - You can specify the `keep` parameter in the `drop_duplicates()` function to control which occurrence to keep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame keeping the last occurrence of duplicates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B   C\n",
       "0  1  5   9\n",
       "2  3  7  11\n",
       "3  2  6  10\n",
       "4  4  8  12"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep the last occurrence of duplicate rows\n",
    "df_last_occurrence = df.drop_duplicates(keep='last')\n",
    "print(\"DataFrame keeping the last occurrence of duplicates:\")\n",
    "df_last_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When handling duplicate entries, consider the following:\n",
    "- Understand the source and nature of the duplicates in your dataset.\n",
    "- Determine whether duplicates are truly redundant or if they carry meaningful information.\n",
    "- Choose the appropriate approach (removing duplicates or keeping specific occurrences) based on your data and analysis requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling duplicate entries ensures data cleanliness and integrity, leading to more accurate and reliable results in data analysis and machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will discuss techniques for handling incorrect or invalid data points in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Handling Incorrect or Invalid Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrect or invalid data refers to data points that violate domain-specific rules, constraints, or logical boundaries. These data points can arise due to various reasons, such as data entry errors, measurement inaccuracies, or system glitches. Examples of incorrect or invalid data include negative age values, invalid zip codes, or temperatures outside the possible range. Identifying and handling such data points is crucial to maintain data quality and ensure the reliability of data analysis and machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying incorrect or invalid data:\n",
    "- Domain knowledge plays a key role in identifying incorrect or invalid data points.\n",
    "- Define specific rules or constraints based on the characteristics and requirements of your data.\n",
    "- Use conditional statements or data validation libraries to check for violations of these rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with incorrect or invalid data\n",
    "df = pd.DataFrame({'Age': [25, -10, 30, 45],\n",
    "                   'Zip_Code': ['12345', '98765', '12345', 'ABCDE'],\n",
    "                   'Temperature': [25.5, 30.2, -5.0, 42.8]})\n",
    "\n",
    "# Define rules for identifying incorrect or invalid data\n",
    "def is_valid_age(age):\n",
    "    return age >= 0\n",
    "\n",
    "def is_valid_zip_code(zip_code):\n",
    "    return zip_code.isdigit() and len(zip_code) == 5\n",
    "\n",
    "def is_valid_temperature(temperature):\n",
    "    return 0 <= temperature <= 50.0\n",
    "\n",
    "# Identify incorrect or invalid data points\n",
    "invalid_age = df['Age'].apply(lambda x: not is_valid_age(x))\n",
    "invalid_zip_code = df['Zip_Code'].apply(lambda x: not is_valid_zip_code(x))\n",
    "invalid_temperature = df['Temperature'].apply(lambda x: not is_valid_temperature(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid age values:\n",
      "   Age Zip_Code  Temperature\n",
      "1  -10    98765         30.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Invalid age values:\")\n",
    "print(df[invalid_age])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid zip codes:\n",
      "   Age Zip_Code  Temperature\n",
      "3   45    ABCDE         42.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Invalid zip codes:\")\n",
    "print(df[invalid_zip_code])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid temperature values:\n",
      "   Age Zip_Code  Temperature\n",
      "2   30    12345         -5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Invalid temperature values:\")\n",
    "print(df[invalid_temperature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling incorrect or invalid data:\n",
    "1. Data correction\n",
    "   - If the incorrect or invalid data points are identifiable and can be corrected based on domain knowledge or additional information, you can update the values accordingly.\n",
    "   - This approach is suitable when the correct values are known or can be inferred from other reliable sources.\n",
    "\n",
    "2. Data removal\n",
    "   - If the incorrect or invalid data points are rare and their removal does not significantly impact the analysis, you can choose to remove those data points from the dataset.\n",
    "   - Be cautious when removing data points, as it may introduce bias or loss of information.\n",
    "\n",
    "3. Data imputation\n",
    "   - In some cases, you can treat incorrect or invalid data points as missing values and apply appropriate imputation techniques, such as mean, median, or mode imputation.\n",
    "   - Imputation allows you to retain the data points while replacing the incorrect or invalid values with estimated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When handling incorrect or invalid data, consider the following:\n",
    "- Establish clear rules and constraints to identify incorrect or invalid data points based on domain knowledge.\n",
    "- Determine the appropriate handling technique (correction, removal, or imputation) based on the nature of the data and the impact on the analysis.\n",
    "- Document the steps taken to handle incorrect or invalid data to ensure transparency and reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling incorrect or invalid data is an important aspect of data preprocessing to ensure the quality and reliability of the data used for analysis and machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will discuss techniques for handling imbalanced datasets, where the distribution of classes or categories is uneven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Handling Imbalanced Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data refers to datasets where the distribution of classes or categories is uneven, with some classes having significantly more instances than others. Imbalanced datasets are common in various domains, such as fraud detection, medical diagnosis, or customer churn prediction, where the minority class is often the class of interest. When training machine learning models on imbalanced data, the models may be biased towards the majority class and perform poorly on the underrepresented minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding imbalanced datasets:\n",
    "- Imbalanced datasets can be characterized by the class distribution ratio, which is the ratio of the number of instances in the majority class to the number of instances in the minority class.\n",
    "- Imbalanced datasets pose challenges for machine learning algorithms, as they tend to optimize for overall accuracy, which may not be suitable when the minority class is of primary importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techniques for handling imbalanced data:\n",
    "\n",
    "1. Oversampling\n",
    "   - Oversampling involves increasing the number of instances in the minority class to balance the class distribution.\n",
    "   - Random oversampling duplicates existing minority class instances randomly.\n",
    "   - Synthetic Minority Over-sampling Technique (SMOTE) generates synthetic examples by interpolating between existing minority class instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Random Oversampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from imbalanced-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from imbalanced-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from imbalanced-learn) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from imbalanced-learn) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset:\n",
      "[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [7, 8]]\n",
      "[0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Create a sample imbalanced dataset\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n",
    "y = [0, 0, 0, 1, 1]  # Minority class: 1, Majority class: 0\n",
    "\n",
    "# Create a RandomOverSampler object\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Perform random oversampling\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "print(\"Resampled dataset:\")\n",
    "print(X_resampled)\n",
    "print(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Undersampling\n",
    "   - Undersampling involves reducing the number of instances in the majority class to balance the class distribution.\n",
    "   - Random undersampling randomly removes instances from the majority class.\n",
    "   - Cluster Centroids undersampling replaces majority class instances with centroids of clusters formed by majority class instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Random Undersampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset:\n",
      "[[1, 2], [3, 4], [7, 8], [9, 10]]\n",
      "[0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Create a sample imbalanced dataset\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n",
    "y = [0, 0, 0, 1, 1]  # Minority class: 1, Majority class: 0\n",
    "\n",
    "# Create a RandomUnderSampler object\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Perform random undersampling\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "print(\"Resampled dataset:\")\n",
    "print(X_resampled)\n",
    "print(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Hybrid methods\n",
    "   - Hybrid methods combine oversampling and undersampling techniques to balance the class distribution.\n",
    "   - SMOTE + Tomek Links applies SMOTE oversampling followed by Tomek Links undersampling to remove overlapping instances.\n",
    "   - SMOTE + ENN (Edited Nearest Neighbors) applies SMOTE oversampling followed by ENN undersampling to remove instances that are misclassified by their nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When handling imbalanced data, consider the following:\n",
    "- Evaluate the class distribution and determine the extent of imbalance in your dataset.\n",
    "- Choose an appropriate technique (oversampling, undersampling, or hybrid) based on the characteristics of your data and the specific requirements of your problem.\n",
    "- Use appropriate evaluation metrics, such as precision, recall, F1-score, or area under the precision-recall curve (AUPRC), which are more suitable for imbalanced datasets than accuracy.\n",
    "- Consider using ensemble methods or cost-sensitive learning approaches that can handle imbalanced data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling imbalanced data is crucial to ensure that machine learning models are not biased towards the majority class and can effectively identify and classify instances of the minority class, which is often the class of interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will discuss techniques for data integration and aggregation, which involve combining data from multiple sources or summarizing data at different levels of granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we explored the importance of data preprocessing and cleaning in machine learning. We discussed various techniques to handle common data quality issues and prepare the data for effective analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key takeaways from the lecture:\n",
    "- Data preprocessing and cleaning are crucial steps in the machine learning pipeline to ensure data quality, consistency, and reliability.\n",
    "- Identifying and handling missing data is essential, and techniques such as deletion and imputation can be used to address missing values.\n",
    "- Outliers are data points that significantly deviate from the majority of the data and can be handled using techniques like deletion, transformation, or winsorization.\n",
    "- Data transformation techniques, including normalization, standardization, and encoding categorical variables, help prepare the data for machine learning algorithms.\n",
    "- Handling imbalanced datasets is important to prevent biased models and ensure effective performance on minority classes. Techniques such as oversampling, undersampling, and hybrid methods can be used to balance the class distribution.\n",
    "- Data integration and aggregation involve combining data from multiple sources and summarizing data at different levels of granularity to facilitate analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the lecture, we provided hands-on examples using Python and popular libraries such as NumPy, Pandas, and Scikit-learn to demonstrate the practical application of data preprocessing techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
