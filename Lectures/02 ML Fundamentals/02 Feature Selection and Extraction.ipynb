{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the quality and relevance of the features used to train models play a crucial role in determining the performance and generalization ability of the models. Feature selection and extraction are two important techniques that help in identifying and creating the most informative and discriminative features from raw data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection** refers to the process of selecting a subset of relevant features from the original set of features. The goal is to reduce the dimensionality of the data by discarding irrelevant or redundant features, while retaining those that are most informative for the task at hand. Feature selection can be performed using various statistical, model-based, or domain-specific criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**, on the other hand, involves transforming the original features into a lower-dimensional space using techniques such as dimensionality reduction or projection. The goal is to create new features that capture the most important information in the data while reducing its complexity. Feature extraction can be unsupervised or supervised, depending on whether the target variable is considered during the transformation process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feature-selection-extraction.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both feature selection and extraction are essential steps in the machine learning pipeline, as they can lead to several benefits, including:\n",
    "\n",
    "1. **Improved Model Performance**: By selecting the most relevant features and discarding irrelevant or redundant ones, we can improve the performance of machine learning models. This is because the models can focus on the most informative aspects of the data, leading to better generalization and accuracy.\n",
    "\n",
    "2. **Reduced Overfitting**: When working with high-dimensional data, there is a risk of overfitting, where the model learns noise or irrelevant patterns in the training data. Feature selection helps in reducing the dimensionality of the data, mitigating the risk of overfitting and improving the model's ability to generalize to unseen data.\n",
    "\n",
    "3. **Faster Training and Inference**: With fewer features, the training and inference times of machine learning models can be significantly reduced. This is especially important when dealing with large datasets or complex models, as it can lead to faster experimentation and deployment.\n",
    "\n",
    "4. **Better Interpretability**: By selecting a subset of relevant features, we can gain insights into which factors are most important for the problem at hand. This enhances the interpretability of the model and helps in understanding the underlying patterns and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature selection and extraction process typically involves the following steps:\n",
    "\n",
    "1. **Data Preprocessing**: Before applying feature selection or extraction techniques, it is essential to preprocess the data. This may include handling missing values, scaling or normalizing the features, and encoding categorical variables.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - **Filter Methods**: These methods assess the relevance of features based on statistical measures such as correlation, chi-square test, or mutual information. Features are ranked or selected based on their individual relevance to the target variable.\n",
    "   - **Wrapper Methods**: These methods evaluate subsets of features by training and testing a model on each subset. The performance of the model is used as a criterion to select the best subset of features. Examples include recursive feature elimination and forward/backward feature selection.\n",
    "   - **Embedded Methods**: These methods combine feature selection with the model training process. The model itself is used to assess the importance of features during training. Examples include Lasso and Ridge regression, which use regularization to penalize less important features.\n",
    "\n",
    "3. **Feature Extraction**:\n",
    "   - **Unsupervised Methods**: These methods aim to transform the original features into a lower-dimensional space while preserving the most important information. Examples include Principal Component Analysis (PCA) and t-SNE.\n",
    "   - **Supervised Methods**: These methods consider the target variable when creating new features. Examples include Linear Discriminant Analysis (LDA) and supervised autoencoders.\n",
    "\n",
    "4. **Model Training and Evaluation**: After selecting or extracting the most relevant features, the machine learning model is trained using the transformed dataset. The model's performance is evaluated using appropriate metrics and validation techniques to assess the effectiveness of the feature selection/extraction process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection and extraction are iterative processes, and it may be necessary to experiment with different techniques and parameter settings to find the optimal set of features for a given problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will dive deeper into specific feature selection and extraction techniques and explore practical examples using popular machine learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Feature Selection Techniques](#toc1_)    \n",
    "  - [Filter Methods](#toc1_1_)    \n",
    "    - [Pearson's Correlation](#toc1_1_1_)    \n",
    "    - [Chi-Square Test](#toc1_1_2_)    \n",
    "    - [Mutual Information](#toc1_1_3_)    \n",
    "  - [Wrapper Methods](#toc1_2_)    \n",
    "    - [Recursive Feature Elimination (RFE)](#toc1_2_1_)    \n",
    "    - [Forward Feature Selection](#toc1_2_2_)    \n",
    "    - [Backward Feature Elimination](#toc1_2_3_)    \n",
    "  - [Embedded Methods](#toc1_3_)    \n",
    "    - [Lasso Regression (L1 Regularization)](#toc1_3_1_)    \n",
    "    - [Ridge Regression (L2 Regularization)](#toc1_3_2_)    \n",
    "    - [Decision Tree-based Feature Importance](#toc1_3_3_)    \n",
    "- [Feature Extraction Techniques](#toc2_)    \n",
    "  - [Principal Component Analysis (PCA)](#toc2_1_)    \n",
    "  - [Linear Discriminant Analysis (LDA)](#toc2_2_)    \n",
    "  - [t-SNE (t-Distributed Stochastic Neighbor Embedding)](#toc2_3_)    \n",
    "  - [Autoencoders](#toc2_4_)    \n",
    "- [Summary and Takeaways](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Feature Selection Techniques](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection techniques help in identifying the most relevant features from a dataset, discarding irrelevant or redundant ones. There are three main categories of feature selection techniques: filter methods, wrapper methods, and embedded methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feature-selection.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Filter Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter methods assess the relevance of features based on statistical measures, independently of any machine learning algorithm. These methods rank or select features based on their individual relevance to the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_'></a>[Pearson's Correlation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's correlation coefficient measures the linear relationship between two variables. It ranges from -1 to +1, where -1 indicates a strong negative correlation, +1 indicates a strong positive correlation, and 0 indicates no correlation. Features with high absolute correlation values with the target variable are considered more relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass', 'age', 'sibsp', 'parch', 'fare']]\n",
    "y = df['survived']\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=3)\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25616762 12.10732158  1.86906751  0.06322858  3.12497225]\n"
     ]
    }
   ],
   "source": [
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_'></a>[Chi-Square Test](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi-square test is used for categorical features. It measures the dependence between a categorical feature and the target variable. Features with high chi-square values are considered more relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=3)\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.75328826e-02 7.86461550e+01 1.65701432e+00 7.59647047e-02\n",
      " 2.28986385e+02]\n"
     ]
    }
   ],
   "source": [
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_3_'></a>[Mutual Information](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information measures the dependence between two variables, capturing both linear and non-linear relationships. Features with high mutual information scores with the target variable are considered more relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=5)\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.06540033 0.         0.         0.03989511]\n"
     ]
    }
   ],
   "source": [
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Wrapper Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods evaluate subsets of features by training and testing a machine learning model on each subset. The performance of the model is used as a criterion to select the best subset of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods are called \"wrapper\" because they wrap the feature selection process around the machine learning algorithm or model. In other words, the feature selection process is performed by repeatedly training and evaluating the model with different subsets of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the wrapper method works:\n",
    "\n",
    "1. Subset Generation: The wrapper method generates a subset of features from the original feature set.\n",
    "\n",
    "2. Model Training: The machine learning model is trained using the selected subset of features.\n",
    "\n",
    "3. Model Evaluation: The trained model is evaluated using a performance metric, such as accuracy or F1 score, on a validation set or through cross-validation.\n",
    "\n",
    "4. Subset Modification: Based on the model's performance, the wrapper method modifies the subset of features. This modification can involve adding or removing features from the subset.\n",
    "\n",
    "5. Iteration: Steps 2-4 are repeated for different subsets of features until a stopping criterion is met, such as a desired number of features or no further improvement in model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea behind wrapper methods is that they use the machine learning algorithm itself to evaluate the quality of the selected features. The model's performance serves as a proxy for the relevance and usefulness of the features. By wrapping the feature selection process around the model, the wrapper method aims to find the optimal subset of features that maximizes the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, wrapper methods can be computationally expensive because they require training and evaluating the model multiple times for different feature subsets. The computational cost increases with the number of features and the complexity of the machine learning algorithm. Despite the computational overhead, wrapper methods can be effective in finding relevant features and improving model performance, especially when the relationship between features and the target variable is complex and not easily captured by simpler methods like filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_'></a>[Backward Feature Elimination](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward feature elimination starts with all features and iteratively removes the least relevant features one at a time. It stops when removing more features degrades the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stopping criterion determines when the feature elimination process should stop. There are several common stopping criteria used in practice:\n",
    "\n",
    "1. Desired number of features: You can specify a predetermined number of features to keep in the model. The elimination process stops when the specified number of features is reached. For example, if you set the desired number of features to 10, the process will stop when there are 10 features left in the model.\n",
    "\n",
    "2. Performance threshold: You can set a performance threshold based on a specific evaluation metric, such as accuracy, precision, recall, or F1-score. The elimination process stops when the model's performance falls below the specified threshold. For example, you may decide to stop the elimination when the model's accuracy drops below 90%.\n",
    "\n",
    "3. Performance degradation: Instead of setting a fixed performance threshold, you can monitor the change in performance after each feature elimination step. If the performance degrades significantly after removing a feature, you can choose to stop the elimination process. This approach allows you to retain features that have a significant impact on the model's performance.\n",
    "\n",
    "4. Statistical significance: You can use statistical tests to assess the significance of each feature's contribution to the model. The elimination process stops when the remaining features are statistically significant based on a predefined significance level (e.g., p-value < 0.05).\n",
    "\n",
    "5. Cross-validation: You can use cross-validation to evaluate the model's performance at each elimination step. The elimination process stops when the model's performance on the validation set starts to degrade consistently across multiple cross-validation folds.\n",
    "\n",
    "6. Domain knowledge: In some cases, domain expertise can guide the stopping criterion. If you have prior knowledge about the importance of certain features, you may choose to keep them in the model regardless of their statistical significance or impact on performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the choice of the stopping criterion depends on the specific problem, the available data, and the goals of the feature selection process. You may need to experiment with different stopping criteria and evaluate their impact on the model's performance and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, it's recommended to use a separate validation set or cross-validation to assess the model's performance during the feature elimination process. This helps prevent overfitting and ensures that the selected features generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "selector = RFE(estimator=LogisticRegression(), n_features_to_select=3)\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_2_'></a>[Forward Feature Selection](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward feature selection starts with an empty set of features and iteratively adds the most relevant features one at a time. It stops when adding more features does not improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Embedded Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods combine feature selection with the model training process. The model itself is used to assess the importance of features during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods are called \"embedded\" because the feature selection process is embedded within the machine learning algorithm itself. Unlike wrapper methods, which treat the machine learning model as a black box and use its performance to guide the feature selection, embedded methods incorporate feature selection as part of the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In embedded methods, the feature selection and model training are performed simultaneously. The machine learning algorithm itself is responsible for determining the importance or relevance of each feature during the training process. This is typically achieved through regularization techniques or by inherently considering feature importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of how embedded methods work:\n",
    "\n",
    "1. Regularization-based methods:\n",
    "   - Lasso (L1 regularization) and Ridge (L2 regularization) are examples of embedded methods that use regularization techniques.\n",
    "   - These methods add a penalty term to the objective function of the model, which encourages the model to select relevant features and shrink the coefficients of irrelevant features towards zero.\n",
    "   - The regularization term controls the trade-off between fitting the training data and keeping the model simple, effectively performing feature selection.\n",
    "\n",
    "2. Tree-based methods:\n",
    "   - Decision tree-based algorithms, such as Random Forest and Gradient Boosting, have built-in feature importance measures.\n",
    "   - During the training process, these algorithms naturally consider the importance of each feature based on how much it contributes to the overall performance of the model.\n",
    "   - Features that are frequently used for splitting decisions or contribute more to reducing the impurity of the tree nodes are considered more important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of embedded methods is that they perform feature selection during the model training process, making them more computationally efficient compared to wrapper methods. The feature selection is guided by the model's objective function or the algorithm's inherent feature importance measures, ensuring that the selected features are relevant to the specific model being trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the downside of embedded methods is that they are specific to the particular machine learning algorithm being used. The feature selection process is tied to the model's training mechanism, and the selected features may not generalize well to other algorithms. Additionally, the interpretability of the selected features may be less straightforward compared to filter methods, which provide explicit statistical measures for feature relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_1_'></a>[Lasso Regression (L1 Regularization)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression adds an L1 regularization term to the linear regression objective function. The L1 penalty encourages sparsity, driving the coefficients of irrelevant features to zero. Features with non-zero coefficients are considered important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "selected_features = X.columns[lasso.coef_ != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fare'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_2_'></a>[Ridge Regression (L2 Regularization)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression adds an L2 regularization term to the linear regression objective function. The L2 penalty shrinks the coefficients of less important features towards zero, but does not eliminate them completely. Features with larger absolute coefficients are considered more important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "feature_importances = np.abs(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07436907, 0.00864391, 0.0327418 , 0.06740302, 0.00066198])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_3_'></a>[Decision Tree-based Feature Importance](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree-based algorithms, such as Random Forests and Gradient Boosting, can provide feature importance scores based on how much each feature contributes to the reduction of impurity or error in the model. Features with higher importance scores are considered more relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.020812  , 0.45092893, 0.05286833, 0.0559312 , 0.41945954])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few examples of feature selection techniques. The choice of technique depends on the nature of the problem, the type of data, and the machine learning algorithm being used. It is often beneficial to experiment with multiple techniques and compare their results to select the most effective set of features for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Feature Extraction Techniques](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction techniques aim to transform the original features into a new, lower-dimensional space while preserving the most important information. These techniques can be useful when dealing with high-dimensional data or when the original features are not directly suitable for machine learning algorithms. Here, we will discuss four popular feature extraction techniques: PCA, LDA, t-SNE, and Autoencoders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feature-selection-extraction-2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Principal Component Analysis (PCA)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is an unsupervised linear transformation technique that seeks to find a new set of orthogonal features (principal components) that capture the maximum variance in the data. The principal components are ordered by the amount of variance they explain, allowing for dimensionality reduction by selecting the top k components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is useful for visualizing high-dimensional data in a lower-dimensional space, identifying patterns, and reducing the dimensionality of the data while preserving the most important information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Linear Discriminant Analysis (LDA)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a supervised linear transformation technique that finds a new set of features that maximizes the separation between different classes. Unlike PCA, which focuses on capturing the maximum variance, LDA tries to find the directions that maximize the class separability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_transformed = lda.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is particularly useful when dealing with classification problems where the goal is to find features that best discriminate between different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[t-SNE (t-Distributed Stochastic Neighbor Embedding)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a non-linear dimensionality reduction technique that aims to preserve the local structure of the data in the low-dimensional space. It maps the high-dimensional data points to a lower-dimensional space such that similar points are close together and dissimilar points are far apart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30)\n",
    "X_transformed = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is widely used for visualizing high-dimensional data in a 2D or 3D space, as it can reveal interesting patterns and clusters in the data. However, it is computationally expensive and may not always preserve the global structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Autoencoders](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are neural network-based models that learn to compress and reconstruct the input data. They consist of an encoder network that maps the input to a lower-dimensional representation (latent space) and a decoder network that reconstructs the original input from the latent representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/autoencoder.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# You need to install tensorflow to run this code\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 728us/step - loss: 0.6945\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 622us/step - loss: 0.6930\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 613us/step - loss: 0.6927\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 604us/step - loss: 0.6921\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 617us/step - loss: 0.6912\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 616us/step - loss: 0.6896\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 584us/step - loss: 0.6874\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6849\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 649us/step - loss: 0.6825\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 601us/step - loss: 0.6805\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 610us/step - loss: 0.6788\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 626us/step - loss: 0.6773\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 584us/step - loss: 0.6758\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 606us/step - loss: 0.6745\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 594us/step - loss: 0.6733\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.6723\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 918us/step - loss: 0.6713\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 627us/step - loss: 0.6706\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 605us/step - loss: 0.6698\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 566us/step - loss: 0.6690\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 576us/step - loss: 0.6683\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 604us/step - loss: 0.6676\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 589us/step - loss: 0.6672\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 573us/step - loss: 0.6668\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 574us/step - loss: 0.6664\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6660\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 670us/step - loss: 0.6658\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 588us/step - loss: 0.6655\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 585us/step - loss: 0.6652\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 597us/step - loss: 0.6651\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 582us/step - loss: 0.6648\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 575us/step - loss: 0.6646\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 591us/step - loss: 0.6645\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 581us/step - loss: 0.6643\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6641\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.6640\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.6638\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 607us/step - loss: 0.6637\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 608us/step - loss: 0.6636\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 606us/step - loss: 0.6634\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 589us/step - loss: 0.6634\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 592us/step - loss: 0.6632\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 607us/step - loss: 0.6630\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6630\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 651us/step - loss: 0.6629\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 602us/step - loss: 0.6627\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 602us/step - loss: 0.6626\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 609us/step - loss: 0.6625\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 579us/step - loss: 0.6624\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 614us/step - loss: 0.6624\n",
      "32/32 [==============================] - 0s 290us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# A feature table with 200 features\n",
    "X = np.random.rand(1000, 200)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=32)\n",
    "\n",
    "X_transformed = encoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders can learn non-linear transformations of the data and capture complex patterns. They are useful for dimensionality reduction, feature learning, and anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature extraction techniques can be applied depending on the nature of the data and the specific requirements of the problem. It is common to experiment with different techniques and evaluate their effectiveness in the context of the machine learning task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Summary and Takeaways](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we explored the concepts of feature selection and extraction, which are crucial steps in the machine learning pipeline. Let's recap the key points and emphasize the importance of these techniques in ML projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection and extraction play a vital role in the success of machine learning projects:\n",
    "\n",
    "1. They help in improving model performance by focusing on the most informative and relevant features, reducing noise and irrelevant information.\n",
    "\n",
    "2. They reduce the risk of overfitting by eliminating redundant or irrelevant features, enhancing the model's ability to generalize to unseen data.\n",
    "\n",
    "3. They can speed up the training and inference processes by reducing the dimensionality of the data, making the models more computationally efficient.\n",
    "\n",
    "4. They enhance the interpretability of the models by identifying the most important features, providing insights into the underlying patterns and relationships in the data.\n",
    "\n",
    "5. They enable the effective handling of high-dimensional data, which is common in many real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By incorporating feature selection and extraction techniques into your machine learning workflow, you can improve the quality of your models, reduce computational complexity, and gain valuable insights from your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To dive deeper into feature selection and extraction techniques, here are some recommended resources:\n",
    "\n",
    "1. Scikit-learn documentation on feature selection: [https://scikit-learn.org/stable/modules/feature_selection.html](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "2. \"Feature Engineering and Selection: A Practical Approach for Predictive Models\" by Max Kuhn and Kjell Johnson: [https://bookdown.org/max/FES/](https://bookdown.org/max/FES/)\n",
    "\n",
    "3. \"An Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani: [https://www.statlearning.com/](https://www.statlearning.com/)\n",
    "\n",
    "4. \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop: [https://www.springer.com/gp/book/9780387310732](https://www.springer.com/gp/book/9780387310732)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These resources provide in-depth explanations, practical examples, and theoretical foundations for feature selection and extraction techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding and applying these techniques effectively, you can enhance the quality and performance of your machine learning models and tackle real-world problems with greater success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
