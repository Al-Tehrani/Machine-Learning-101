{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Feature Selection and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the quality and relevance of the features used to train models play a crucial role in determining the performance and generalization ability of the models. Feature selection and extraction are two important techniques that help in identifying and creating the most informative and discriminative features from raw data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection** refers to the process of selecting a subset of relevant features from the original set of features. The goal is to reduce the dimensionality of the data by discarding irrelevant or redundant features, while retaining those that are most informative for the task at hand. Feature selection can be performed using various statistical, model-based, or domain-specific criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**, on the other hand, involves transforming the original features into a lower-dimensional space using techniques such as dimensionality reduction or projection. The goal is to create new features that capture the most important information in the data while reducing its complexity. Feature extraction can be unsupervised or supervised, depending on whether the target variable is considered during the transformation process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feature-selection-extraction.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both feature selection and extraction are essential steps in the machine learning pipeline, as they can lead to several benefits, including:\n",
    "\n",
    "1. **Improved Model Performance**: By selecting the most relevant features and discarding irrelevant or redundant ones, we can improve the performance of machine learning models. This is because the models can focus on the most informative aspects of the data, leading to better generalization and accuracy.\n",
    "\n",
    "2. **Reduced Overfitting**: When working with high-dimensional data, there is a risk of overfitting, where the model learns noise or irrelevant patterns in the training data. Feature selection helps in reducing the dimensionality of the data, mitigating the risk of overfitting and improving the model's ability to generalize to unseen data.\n",
    "\n",
    "3. **Faster Training and Inference**: With fewer features, the training and inference times of machine learning models can be significantly reduced. This is especially important when dealing with large datasets or complex models, as it can lead to faster experimentation and deployment.\n",
    "\n",
    "4. **Better Interpretability**: By selecting a subset of relevant features, we can gain insights into which factors are most important for the problem at hand. This enhances the interpretability of the model and helps in understanding the underlying patterns and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature selection and extraction process typically involves the following steps:\n",
    "\n",
    "1. **Data Preprocessing**: Before applying feature selection or extraction techniques, it is essential to preprocess the data. This may include handling missing values, scaling or normalizing the features, and encoding categorical variables.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - **Filter Methods**: These methods assess the relevance of features based on statistical measures such as correlation, chi-square test, or mutual information. Features are ranked or selected based on their individual relevance to the target variable.\n",
    "   - **Wrapper Methods**: These methods evaluate subsets of features by training and testing a model on each subset. The performance of the model is used as a criterion to select the best subset of features. Examples include recursive feature elimination and forward/backward feature selection.\n",
    "   - **Embedded Methods**: These methods combine feature selection with the model training process. The model itself is used to assess the importance of features during training. Examples include Lasso and Ridge regression, which use regularization to penalize less important features.\n",
    "\n",
    "3. **Feature Extraction**:\n",
    "   - **Unsupervised Methods**: These methods aim to transform the original features into a lower-dimensional space while preserving the most important information. Examples include Principal Component Analysis (PCA) and t-SNE.\n",
    "   - **Supervised Methods**: These methods consider the target variable when creating new features. Examples include Linear Discriminant Analysis (LDA) and supervised autoencoders.\n",
    "\n",
    "4. **Model Training and Evaluation**: After selecting or extracting the most relevant features, the machine learning model is trained using the transformed dataset. The model's performance is evaluated using appropriate metrics and validation techniques to assess the effectiveness of the feature selection/extraction process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection and extraction are iterative processes, and it may be necessary to experiment with different techniques and parameter settings to find the optimal set of features for a given problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will dive deeper into specific feature selection and extraction techniques and explore practical examples using popular machine learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Feature Selection Techniques](#toc1_)    \n",
    "  - [Filter Methods](#toc1_1_)    \n",
    "    - [Pearson's Correlation](#toc1_1_1_)    \n",
    "    - [Chi-Square Test](#toc1_1_2_)    \n",
    "    - [Mutual Information](#toc1_1_3_)    \n",
    "  - [Wrapper Methods](#toc1_2_)    \n",
    "    - [Recursive Feature Elimination (RFE)](#toc1_2_1_)    \n",
    "    - [Forward Feature Selection](#toc1_2_2_)    \n",
    "    - [Backward Feature Elimination](#toc1_2_3_)    \n",
    "  - [Embedded Methods](#toc1_3_)    \n",
    "    - [Lasso Regression (L1 Regularization)](#toc1_3_1_)    \n",
    "    - [Ridge Regression (L2 Regularization)](#toc1_3_2_)    \n",
    "    - [Decision Tree-based Feature Importance](#toc1_3_3_)    \n",
    "- [Feature Extraction Techniques](#toc2_)    \n",
    "  - [Principal Component Analysis (PCA)](#toc2_1_)    \n",
    "  - [Linear Discriminant Analysis (LDA)](#toc2_2_)    \n",
    "  - [t-SNE (t-Distributed Stochastic Neighbor Embedding)](#toc2_3_)    \n",
    "  - [Autoencoders](#toc2_4_)    \n",
    "- [Summary and Takeaways](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Feature Selection Techniques](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection techniques help in identifying the most relevant features from a dataset, discarding irrelevant or redundant ones. There are three main categories of feature selection techniques: filter methods, wrapper methods, and embedded methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/feature-selection.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Filter Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter methods assess the relevance of features based on statistical measures, independently of any machine learning algorithm. These methods rank or select features based on their individual relevance to the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_'></a>[Pearson's Correlation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's correlation coefficient measures the linear relationship between two variables. It ranges from -1 to +1, where -1 indicates a strong negative correlation, +1 indicates a strong positive correlation, and 0 indicates no correlation. Features with high absolute correlation values with the target variable are considered more relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass', 'age', 'sibsp', 'parch', 'fare']]\n",
    "y = df['survived']\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=3)\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25616762 12.10732158  1.86906751  0.06322858  3.12497225]\n"
     ]
    }
   ],
   "source": [
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_'></a>[Chi-Square Test](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi-square test is used for categorical features. It measures the dependence between a categorical feature and the target variable. Features with high chi-square values are considered more relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=3)\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.75328826e-02 7.86461550e+01 1.65701432e+00 7.59647047e-02\n",
      " 2.28986385e+02]\n"
     ]
    }
   ],
   "source": [
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_3_'></a>[Mutual Information](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information measures the dependence between two variables, capturing both linear and non-linear relationships. Features with high mutual information scores with the target variable are considered more relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=5)\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03945778 0.0519945  0.03532459 0.01871036 0.05609225]\n"
     ]
    }
   ],
   "source": [
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Wrapper Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods evaluate subsets of features by training and testing a machine learning model on each subset. The performance of the model is used as a criterion to select the best subset of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_1_'></a>[Recursive Feature Elimination (RFE)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE recursively removes the least important features based on a specified machine learning algorithm. It starts with all features and iteratively eliminates the least important ones until the desired number of features is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "selector = RFE(estimator=LogisticRegression(), n_features_to_select=3)\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_2_'></a>[Forward Feature Selection](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward feature selection starts with an empty set of features and iteratively adds the most relevant features one at a time. It stops when adding more features does not improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_'></a>[Backward Feature Elimination](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward feature elimination starts with all features and iteratively removes the least relevant features one at a time. It stops when removing more features degrades the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Embedded Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods combine feature selection with the model training process. The model itself is used to assess the importance of features during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_1_'></a>[Lasso Regression (L1 Regularization)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression adds an L1 regularization term to the linear regression objective function. The L1 penalty encourages sparsity, driving the coefficients of irrelevant features to zero. Features with non-zero coefficients are considered important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "selected_features = X.columns[lasso.coef_ != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fare'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_2_'></a>[Ridge Regression (L2 Regularization)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression adds an L2 regularization term to the linear regression objective function. The L2 penalty shrinks the coefficients of less important features towards zero, but does not eliminate them completely. Features with larger absolute coefficients are considered more important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "feature_importances = np.abs(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07436907, 0.00864391, 0.0327418 , 0.06740302, 0.00066198])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_3_'></a>[Decision Tree-based Feature Importance](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree-based algorithms, such as Random Forests and Gradient Boosting, can provide feature importance scores based on how much each feature contributes to the reduction of impurity or error in the model. Features with higher importance scores are considered more relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02328554, 0.4391113 , 0.0492425 , 0.06215571, 0.42620495])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few examples of feature selection techniques. The choice of technique depends on the nature of the problem, the type of data, and the machine learning algorithm being used. It is often beneficial to experiment with multiple techniques and compare their results to select the most effective set of features for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Feature Extraction Techniques](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction techniques aim to transform the original features into a new, lower-dimensional space while preserving the most important information. These techniques can be useful when dealing with high-dimensional data or when the original features are not directly suitable for machine learning algorithms. Here, we will discuss four popular feature extraction techniques: PCA, LDA, t-SNE, and Autoencoders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Principal Component Analysis (PCA)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is an unsupervised linear transformation technique that seeks to find a new set of orthogonal features (principal components) that capture the maximum variance in the data. The principal components are ordered by the amount of variance they explain, allowing for dimensionality reduction by selecting the top k components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is useful for visualizing high-dimensional data in a lower-dimensional space, identifying patterns, and reducing the dimensionality of the data while preserving the most important information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Linear Discriminant Analysis (LDA)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a supervised linear transformation technique that finds a new set of features that maximizes the separation between different classes. Unlike PCA, which focuses on capturing the maximum variance, LDA tries to find the directions that maximize the class separability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_transformed = lda.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is particularly useful when dealing with classification problems where the goal is to find features that best discriminate between different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[t-SNE (t-Distributed Stochastic Neighbor Embedding)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a non-linear dimensionality reduction technique that aims to preserve the local structure of the data in the low-dimensional space. It maps the high-dimensional data points to a lower-dimensional space such that similar points are close together and dissimilar points are far apart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30)\n",
    "X_transformed = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is widely used for visualizing high-dimensional data in a 2D or 3D space, as it can reveal interesting patterns and clusters in the data. However, it is computationally expensive and may not always preserve the global structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_'></a>[Autoencoders](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are neural network-based models that learn to compress and reconstruct the input data. They consist of an encoder network that maps the input to a lower-dimensional representation (latent space) and a decoder network that reconstructs the original input from the latent representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 0s 918us/step - loss: -58.9551\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 630us/step - loss: -128.5828\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 678us/step - loss: -199.9322\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 586us/step - loss: -272.6886\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 619us/step - loss: -344.4696\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 648us/step - loss: -416.7390\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 626us/step - loss: -492.2270\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 597us/step - loss: -570.1879\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 548us/step - loss: -649.8934\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 620us/step - loss: -740.5331\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -837.0887\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -933.0208\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -1031.2573\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -1143.9954\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -1253.7964\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 517us/step - loss: -1372.5752\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 630us/step - loss: -1488.9152\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 835us/step - loss: -1631.3535\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 611us/step - loss: -1772.9347\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 614us/step - loss: -1921.9720\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 612us/step - loss: -2074.3081\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 549us/step - loss: -2231.3735\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 522us/step - loss: -2403.7251\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 628us/step - loss: -2579.3108\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 470us/step - loss: -2764.2949\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 513us/step - loss: -2970.4985\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 490us/step - loss: -3169.7676\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 485us/step - loss: -3384.5605\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 639us/step - loss: -3610.2837\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 616us/step - loss: -3831.7561\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 491us/step - loss: -4094.1396\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 533us/step - loss: -4328.1787\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 525us/step - loss: -4587.6914\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 589us/step - loss: -4870.5605\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 463us/step - loss: -5113.8477\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 459us/step - loss: -5414.8999\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 608us/step - loss: -5704.5430\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 501us/step - loss: -6025.1270\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 471us/step - loss: -6367.8418\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 509us/step - loss: -6655.0591\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 541us/step - loss: -6989.0981\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 488us/step - loss: -7356.6079\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 514us/step - loss: -7721.2754\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 504us/step - loss: -8069.2866\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 577us/step - loss: -8450.7168\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 525us/step - loss: -8813.5977\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 483us/step - loss: -9214.2578\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 458us/step - loss: -9602.7510\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 554us/step - loss: -10004.9756\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 901us/step - loss: -10410.3457\n",
      "6/6 [==============================] - 0s 445us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=32)\n",
    "\n",
    "X_transformed = encoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders can learn non-linear transformations of the data and capture complex patterns. They are useful for dimensionality reduction, feature learning, and anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature extraction techniques can be applied depending on the nature of the data and the specific requirements of the problem. It is common to experiment with different techniques and evaluate their effectiveness in the context of the machine learning task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Summary and Takeaways](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we explored the concepts of feature selection and extraction, which are crucial steps in the machine learning pipeline. Let's recap the key points and emphasize the importance of these techniques in ML projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection and extraction play a vital role in the success of machine learning projects:\n",
    "\n",
    "1. They help in improving model performance by focusing on the most informative and relevant features, reducing noise and irrelevant information.\n",
    "\n",
    "2. They reduce the risk of overfitting by eliminating redundant or irrelevant features, enhancing the model's ability to generalize to unseen data.\n",
    "\n",
    "3. They can speed up the training and inference processes by reducing the dimensionality of the data, making the models more computationally efficient.\n",
    "\n",
    "4. They enhance the interpretability of the models by identifying the most important features, providing insights into the underlying patterns and relationships in the data.\n",
    "\n",
    "5. They enable the effective handling of high-dimensional data, which is common in many real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By incorporating feature selection and extraction techniques into your machine learning workflow, you can improve the quality of your models, reduce computational complexity, and gain valuable insights from your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To dive deeper into feature selection and extraction techniques, here are some recommended resources:\n",
    "\n",
    "1. Scikit-learn documentation on feature selection: [https://scikit-learn.org/stable/modules/feature_selection.html](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "2. \"Feature Engineering and Selection: A Practical Approach for Predictive Models\" by Max Kuhn and Kjell Johnson: [https://bookdown.org/max/FES/](https://bookdown.org/max/FES/)\n",
    "\n",
    "3. \"An Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani: [https://www.statlearning.com/](https://www.statlearning.com/)\n",
    "\n",
    "4. \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop: [https://www.springer.com/gp/book/9780387310732](https://www.springer.com/gp/book/9780387310732)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These resources provide in-depth explanations, practical examples, and theoretical foundations for feature selection and extraction techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding and applying these techniques effectively, you can enhance the quality and performance of your machine learning models and tackle real-world problems with greater success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
