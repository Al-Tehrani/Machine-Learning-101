{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, evaluating the performance of a model is crucial to understanding its effectiveness and making informed decisions about its deployment. Model evaluation metrics provide quantitative measures to assess how well a model performs on a given task. These metrics help us compare different models, tune hyperparameters, and select the best model for our specific problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the appropriate evaluation metric depends on the type of machine learning problem you are solving, such as regression, classification, clustering, or reinforcement learning. Each type of problem has its own set of evaluation metrics that are commonly used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we will explore various evaluation metrics for different types of machine learning problems, including:\n",
    "\n",
    "- Supervised Learning\n",
    "  - Regression Metrics\n",
    "  - Classification Metrics\n",
    "  - Multilabel Classification Metrics\n",
    "- Unsupervised Learning\n",
    "  - Clustering Metrics\n",
    "  - Dimensionality Reduction Metrics\n",
    "- Reinforcement Learning Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also discuss model selection and validation techniques, such as cross-validation, to ensure the robustness and generalizability of our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this lecture, you will have a solid understanding of the importance of model evaluation metrics and how to select the appropriate metric for your specific machine learning problem. You will also gain practical knowledge of how to implement and interpret these metrics using code examples in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Supervised Learning Metrics](#toc1_)    \n",
    "  - [Regression Metrics](#toc1_1_)    \n",
    "    - [Mean Absolute Error (MAE)](#toc1_1_1_)    \n",
    "    - [Mean Squared Error (MSE)](#toc1_1_2_)    \n",
    "    - [Root Mean Squared Error (RMSE)](#toc1_1_3_)    \n",
    "    - [R-squared (R²)](#toc1_1_4_)    \n",
    "    - [Adjusted R-squared](#toc1_1_5_)    \n",
    "  - [Classification Metrics](#toc1_2_)    \n",
    "    - [Accuracy](#toc1_2_1_)    \n",
    "    - [Precision](#toc1_2_2_)    \n",
    "    - [Recall](#toc1_2_3_)    \n",
    "    - [F1-score](#toc1_2_4_)    \n",
    "    - [Confusion Matrix](#toc1_2_5_)    \n",
    "    - [ROC Curve and AUC (Area Under the Curve)](#toc1_2_6_)    \n",
    "- [Unsupervised Learning Metrics](#toc2_)    \n",
    "  - [Clustering Metrics](#toc2_1_)    \n",
    "    - [Silhouette Coefficient](#toc2_1_1_)    \n",
    "    - [Davies-Bouldin Index](#toc2_1_2_)    \n",
    "    - [Calinski-Harabasz Index](#toc2_1_3_)    \n",
    "  - [Dimensionality Reduction Metrics](#toc2_2_)    \n",
    "    - [Reconstruction Error](#toc2_2_1_)    \n",
    "    - [Explained Variance](#toc2_2_2_)    \n",
    "- [Reinforcement Learning Metrics](#toc3_)    \n",
    "    - [Cumulative Reward](#toc3_1_1_)    \n",
    "    - [Average Reward per Episode](#toc3_1_2_)    \n",
    "    - [Average Q-value](#toc3_1_3_)    \n",
    "- [Model Selection and Validation](#toc4_)    \n",
    "    - [Cross-validation](#toc4_1_1_)    \n",
    "    - [Stratified k-fold cross-validation](#toc4_1_2_)    \n",
    "- [Conclusion](#toc5_)    \n",
    "  - [Recap of the importance of model evaluation metrics](#toc5_1_)    \n",
    "  - [Choosing the right metric for your problem](#toc5_2_)    \n",
    "  - [Further reading and resources](#toc5_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Supervised Learning Metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Regression Metrics](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression metrics are used to evaluate the performance of models that predict continuous values. These metrics measure how well the predicted values match the actual values. Let's explore some commonly used regression metrics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/regression-metrics.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_'></a>[Mean Absolute Error (MAE)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE measures the average absolute difference between the predicted and actual values. It gives an idea of the average magnitude of the errors without considering their direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.18\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.2, 1.8, 3.2, 4.1, 4.8]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_'></a>[Mean Squared Error (MSE)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE measures the average squared difference between the predicted and actual values. It penalizes larger errors more heavily than smaller ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.03\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.2, 1.8, 3.2, 4.1, 4.8]\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_3_'></a>[Root Mean Squared Error (RMSE)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is the square root of the MSE. It provides a measure of the average magnitude of the errors, similar to MAE, but with a stronger emphasis on larger errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.18\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.2, 1.8, 3.2, 4.1, 4.8]\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_4_'></a>[R-squared (R²)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1, with 1 indicating a perfect fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.2, 1.8, 3.2, 4.1, 4.8]\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_5_'></a>[Adjusted R-squared](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It penalizes the addition of unnecessary predictors and provides a more accurate measure of the model's goodness of fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.2, 1.8, 3.2, 4.1, 4.8]\n",
    "n_samples = len(y_true)\n",
    "n_features = 1\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "adjusted_r2 = 1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)\n",
    "print(f\"Adjusted R-squared: {adjusted_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These regression metrics provide different perspectives on the model's performance. MAE, MSE, and RMSE focus on the magnitude of the errors, while R-squared and adjusted R-squared measure the goodness of fit. It's important to consider multiple metrics and choose the ones that align with your specific problem and goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Classification Metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification metrics are used to evaluate the performance of models that predict discrete class labels. These metrics measure how well the model distinguishes between different classes. Let's explore some commonly used classification metrics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/classification-metrics.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_1_'></a>[Accuracy](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy measures the proportion of correct predictions among the total number of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_pred = [1, 1, 1, 0, 0]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_2_'></a>[Precision](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision measures the proportion of true positive predictions among all positive predictions. It answers the question: \"Out of all instances predicted as positive, how many are actually positive?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_pred = [1, 1, 1, 0, 0]\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_'></a>[Recall](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, also known as sensitivity or true positive rate, measures the proportion of true positive predictions among all actual positive instances. It answers the question: \"Out of all actual positive instances, how many are correctly predicted as positive?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_pred = [1, 1, 1, 0, 0]\n",
    "\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_4_'></a>[F1-score](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of a model's performance, especially when the classes are imbalanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_pred = [1, 1, 1, 0, 0]\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_5_'></a>[Confusion Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a tabular summary of the model's performance, showing the counts of true positive, true negative, false positive, and false negative predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1 1]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_pred = [1, 1, 1, 0, 0]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_6_'></a>[ROC Curve and AUC (Area Under the Curve)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve plots the true positive rate (recall) against the false positive rate at various threshold settings. AUC measures the area under the ROC curve, providing an aggregate measure of the model's performance across all possible classification thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_pred_proba = [0.8, 0.6, 0.7, 0.4, 0.3]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classification metrics provide different perspectives on the model's performance. Accuracy gives an overall measure of correctness, while precision and recall focus on the model's performance for the positive class. The F1-score combines precision and recall into a single metric. The confusion matrix provides a detailed breakdown of the model's predictions, and the ROC curve and AUC assess the model's performance across different classification thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to consider the characteristics of your problem, such as class imbalance, and choose the metrics that align with your specific goals. Additionally, visualizing the ROC curve can provide insights into the trade-off between true positive rate and false positive rate at different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Unsupervised Learning Metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Clustering Metrics](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering metrics are used to evaluate the quality of clustering results in unsupervised learning. These metrics assess the compactness and separation of clusters, as well as the overall goodness of the clustering structure. Let's explore some commonly used clustering metrics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/clustering-metrics.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_'></a>[Silhouette Coefficient](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient measures the compactness and separation of clusters. It ranges from -1 to 1, where a higher value indicates better-defined clusters. The Silhouette Coefficient considers both the intra-cluster distance (how close points are to other points within the same cluster) and the inter-cluster distance (how far points are from points in other clusters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = [[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]]\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Coefficient: {silhouette_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_2_'></a>[Davies-Bouldin Index](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Davies-Bouldin Index measures the average similarity between clusters, where similarity is defined as the ratio of within-cluster distances to between-cluster distances. A lower Davies-Bouldin Index indicates better clustering, with well-separated and compact clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin Index: 0.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = [[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]]\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "db_index = davies_bouldin_score(X, labels)\n",
    "print(f\"Davies-Bouldin Index: {db_index:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_3_'></a>[Calinski-Harabasz Index](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Calinski-Harabasz Index, also known as the Variance Ratio Criterion, measures the ratio of between-cluster dispersion to within-cluster dispersion. A higher Calinski-Harabasz Index indicates better-defined clusters, with good separation between clusters and compactness within clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabasz Index: 35.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = [[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]]\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "ch_index = calinski_harabasz_score(X, labels)\n",
    "print(f\"Calinski-Harabasz Index: {ch_index:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These clustering metrics provide different perspectives on the quality of the clustering results. The Silhouette Coefficient considers both the compactness and separation of clusters, giving an overall measure of the clustering structure. The Davies-Bouldin Index focuses on the similarity between clusters, penalizing clusters that are too close to each other. The Calinski-Harabasz Index measures the ratio of between-cluster dispersion to within-cluster dispersion, favoring well-separated and compact clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that these metrics have their own assumptions and limitations. They may be sensitive to the shape, size, and density of clusters, as well as the presence of noise or outliers. Therefore, it's recommended to use multiple metrics and visualize the clustering results to gain a comprehensive understanding of the clustering quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the interpretation of these metrics may vary depending on the specific problem and domain. It's crucial to consider the characteristics of your data and the goals of your clustering analysis when selecting and interpreting clustering metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[Dimensionality Reduction Metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques aim to reduce the number of features while preserving the essential information in the data. Evaluating the quality of dimensionality reduction results is important to ensure that the reduced representation captures the relevant patterns and minimizes information loss. Let's explore two commonly used dimensionality reduction metrics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/dimensionality-reduction-metrics.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_1_'></a>[Reconstruction Error](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction error measures the difference between the original data and the reconstructed data after dimensionality reduction. It quantifies the information loss incurred during the reduction process. A lower reconstruction error indicates better preservation of the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Error: 0.07\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(100, 20)\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reconstructed = pca.inverse_transform(X_reduced)\n",
    "\n",
    "reconstruction_error = mean_squared_error(X, X_reconstructed)\n",
    "print(f\"Reconstruction Error: {reconstruction_error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use Principal Component Analysis (PCA) to reduce the dimensionality of the data from 3 to 2 components. We then reconstruct the original data using the reduced representation and calculate the reconstruction error using mean squared error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_2_'></a>[Explained Variance](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained variance measures the proportion of the total variance in the data that is captured by each principal component. It helps determine the number of components required to retain a desired level of information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio:\n",
      "Principal Component 1: 1.00\n",
      "Principal Component 2: 0.00\n",
      "Principal Component 3: 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\")\n",
    "for i, ratio in enumerate(explained_variance_ratio):\n",
    "    print(f\"Principal Component {i+1}: {ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we apply PCA to the data and calculate the explained variance ratio for each principal component. The explained variance ratio indicates the proportion of the total variance explained by each component. We can see that the first principal component captures 99% of the variance, while the second and third components capture the remaining 1% and 0%, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dimensionality reduction metrics provide insights into the quality of the reduced representation. Reconstruction error quantifies the information loss during the reduction process, with a lower error indicating better preservation of the original data. Explained variance helps determine the number of components needed to retain a desired level of information, allowing us to strike a balance between dimensionality reduction and information preservation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to consider the specific requirements of your problem when selecting dimensionality reduction techniques and evaluating their performance. The choice of the number of components depends on factors such as the desired level of information retention, computational efficiency, and interpretability of the reduced representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, visualizing the reduced data using techniques like scatter plots or heatmaps can provide qualitative insights into the structure and patterns captured by the dimensionality reduction process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that dimensionality reduction is an exploratory technique, and the metrics discussed here are just a few examples. Other metrics, such as silhouette score or t-SNE perplexity, can also be used depending on the specific algorithm and the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Reinforcement Learning Metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning involves an agent learning to make decisions by interacting with an environment to maximize a reward signal. Evaluating the performance of reinforcement learning algorithms requires specialized metrics that capture the agent's learning progress and the quality of its decision-making. Let's explore some commonly used reinforcement learning metrics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_'></a>[Cumulative Reward](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cumulative reward is the sum of rewards obtained by the agent over a certain number of time steps or episodes. It measures the overall performance of the agent in accumulating rewards throughout its interaction with the environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def evaluate_cumulative_reward(env, agent, num_episodes):\n",
    "    cumulative_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        cumulative_rewards.append(episode_reward)\n",
    "    return cumulative_rewards\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = YourRLAgent()  # Replace with your actual agent implementation\n",
    "num_episodes = 100\n",
    "\n",
    "cumulative_rewards = evaluate_cumulative_reward(env, agent, num_episodes)\n",
    "print(f\"Cumulative Rewards: {cumulative_rewards}\")\n",
    "print(f\"Average Cumulative Reward: {np.mean(cumulative_rewards):.2f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a function `evaluate_cumulative_reward` that evaluates the cumulative reward of an agent over a specified number of episodes. The agent interacts with the environment, and the rewards obtained in each episode are accumulated. Finally, we calculate the average cumulative reward across all episodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_'></a>[Average Reward per Episode](#toc0_)\n",
    "Average reward per episode measures the average reward obtained by the agent in each episode. It provides an indication of the agent's performance on a per-episode basis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def evaluate_average_reward(env, agent, num_episodes):\n",
    "    episode_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return np.mean(episode_rewards)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = YourRLAgent()  # Replace with your actual agent implementation\n",
    "num_episodes = 100\n",
    "\n",
    "average_reward = evaluate_average_reward(env, agent, num_episodes)\n",
    "print(f\"Average Reward per Episode: {average_reward:.2f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a function `evaluate_average_reward` that evaluates the average reward per episode. The agent interacts with the environment for a specified number of episodes, and the rewards obtained in each episode are recorded. Finally, we calculate the average reward across all episodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_3_'></a>[Average Q-value](#toc0_)\n",
    "Average Q-value is a metric specific to Q-learning algorithms. It measures the average estimated Q-value of the actions taken by the agent. Higher Q-values indicate that the agent has learned to assign higher values to favorable actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def evaluate_average_q_value(env, agent, num_episodes):\n",
    "    q_values = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            q_value = agent.get_q_value(state, action)\n",
    "            q_values.append(q_value)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "    return np.mean(q_values)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = YourQLearningAgent()  # Replace with your actual Q-learning agent implementation\n",
    "num_episodes = 100\n",
    "\n",
    "average_q_value = evaluate_average_q_value(env, agent, num_episodes)\n",
    "print(f\"Average Q-value: {average_q_value:.2f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a function `evaluate_average_q_value` that evaluates the average Q-value of the actions taken by the agent. The agent interacts with the environment for a specified number of episodes, and the Q-values of the selected actions are recorded. Finally, we calculate the average Q-value across all recorded values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These reinforcement learning metrics provide different perspectives on the agent's performance. Cumulative reward captures the overall performance in accumulating rewards, average reward per episode measures the agent's performance on a per-episode basis, and average Q-value is specific to Q-learning algorithms and indicates the quality of the learned action-value estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that these metrics are just a few examples, and the choice of metrics depends on the specific reinforcement learning algorithm and the problem at hand. Other metrics, such as the number of steps per episode, success rate, or time to reach a goal state, can also be used to evaluate the agent's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting these metrics, it's crucial to consider the characteristics of the environment, the complexity of the task, and the specific goals of the reinforcement learning problem. Comparing the metrics across different agents or algorithmic variations can provide insights into their relative performance and help in selecting the most suitable approach for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Model Selection and Validation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection and validation are crucial steps in the machine learning workflow to ensure the generalization performance of models and prevent overfitting. These techniques help in selecting the best model architecture, hyperparameters, and assessing the model's performance on unseen data. Let's explore two commonly used model selection and validation techniques:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_1_'></a>[Cross-validation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique that involves splitting the data into multiple subsets, training and evaluating the model on different combinations of these subsets, and aggregating the results to obtain a more robust performance estimate. The most common variant is k-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/k-fold.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "Average cross-validation score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Average cross-validation score: {np.mean(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the `cross_val_score` function from scikit-learn to perform 5-fold cross-validation on a support vector machine (SVM) classifier. The data is split into 5 folds, and the model is trained and evaluated on different combinations of these folds. The resulting scores for each fold are reported, along with the average cross-validation score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation helps in assessing the model's performance on different subsets of the data, providing a more reliable estimate of its generalization ability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_1_2_'></a>[Stratified k-fold cross-validation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified k-fold cross-validation is a variant of k-fold cross-validation that ensures the class distribution in each fold is representative of the overall class distribution in the dataset. This is particularly useful when dealing with imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified k-fold cross-validation scores: [1.0, 1.0, 0.9333333333333333, 1.0, 1.0]\n",
      "Average stratified k-fold cross-validation score: 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Stratified k-fold cross-validation scores: {scores}\")\n",
    "print(f\"Average stratified k-fold cross-validation score: {np.mean(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the `StratifiedKFold` class from scikit-learn to perform stratified 5-fold cross-validation. The `split` method is used to generate the train and test indices for each fold, ensuring that the class distribution is maintained. We iterate over the folds, train the model on the training set, make predictions on the test set, and calculate the accuracy score for each fold. Finally, we report the scores for each fold and the average stratified k-fold cross-validation score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified k-fold cross-validation is particularly useful when the class distribution is imbalanced, as it ensures that each fold contains a representative proportion of samples from each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These model selection and validation techniques help in assessing the model's performance, comparing different models, and selecting the best one for the given task. They provide a more reliable estimate of the model's generalization ability and help in preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the choice of the number of folds (k) depends on the size of the dataset and the computational resources available. A common choice is k=5 or k=10, but it can be adjusted based on the specific requirements of the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, other model selection techniques, such as hold-out validation, leave-one-out cross-validation, or nested cross-validation, can be used depending on the characteristics of the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Conclusion](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we have explored the importance of model evaluation metrics in assessing the performance of machine learning models. We covered various metrics for different types of machine learning problems, including supervised learning (regression, classification, multilabel classification), unsupervised learning (clustering, dimensionality reduction), and reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Recap of the importance of model evaluation metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation metrics provide quantitative measures to assess how well a model performs on a given task. They help in comparing different models, selecting the best model for the problem at hand, and making informed decisions about model deployment. Without proper evaluation metrics, it would be challenging to determine the effectiveness and reliability of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics allow us to:\n",
    "- Assess the model's performance on unseen data\n",
    "- Compare different models and select the best one\n",
    "- Identify areas for improvement and optimize the model\n",
    "- Communicate the model's performance to stakeholders\n",
    "- Monitor the model's performance over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_'></a>[Choosing the right metric for your problem](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the appropriate evaluation metric is crucial for effectively assessing the model's performance. The choice of metric depends on the specific problem, the characteristics of the data, and the goals of the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For supervised learning problems:\n",
    "- Regression metrics, such as MAE, MSE, RMSE, and R-squared, are used to evaluate the model's ability to predict continuous values.\n",
    "- Classification metrics, such as accuracy, precision, recall, F1-score, and ROC AUC, are used to assess the model's performance in predicting discrete class labels.\n",
    "- Multilabel classification metrics, such as micro-averaging, macro-averaging, and weighted-averaging, are used when instances can belong to multiple classes simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unsupervised learning problems:\n",
    "- Clustering metrics, such as Silhouette Coefficient, Davies-Bouldin Index, and Calinski-Harabasz Index, are used to evaluate the quality of clustering results.\n",
    "- Dimensionality reduction metrics, such as reconstruction error and explained variance, are used to assess the quality of the reduced representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reinforcement learning problems:\n",
    "- Metrics such as cumulative reward, average reward per episode, and average Q-value are used to evaluate the agent's learning progress and decision-making quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to consider multiple metrics and understand their strengths and limitations to gain a comprehensive understanding of the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_3_'></a>[Further reading and resources](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To dive deeper into model evaluation metrics and their applications, here are some recommended resources:\n",
    "\n",
    "- Scikit-learn documentation on model evaluation: [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- \"Evaluating Machine Learning Models\" by Alice Zheng: [https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/](https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/)\n",
    "- \"Machine Learning Mastery\" blog by Jason Brownlee: [https://machinelearningmastery.com/](https://machinelearningmastery.com/)\n",
    "- \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron: [https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, model evaluation is an iterative process. It involves selecting appropriate metrics, assessing the model's performance, and refining the model based on the insights gained. By understanding and applying the right evaluation metrics, you can make informed decisions and build reliable and effective machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
